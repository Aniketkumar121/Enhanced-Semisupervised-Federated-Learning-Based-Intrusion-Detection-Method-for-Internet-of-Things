{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7183381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy==1.26.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: matplotlib==3.8.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: pandas==2.1.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from torch==2.1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from torch==2.1.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from torch==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from torch==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from torch==2.1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from torch==2.1.0) (2025.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from scikit-learn==1.3.0) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from scikit-learn==1.3.0) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from scikit-learn==1.3.0) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from matplotlib==3.8.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from pandas==2.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from pandas==2.1.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.8.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\iotworkstation\\desktop\\majorproject\\ussfl\\venv\\lib\\site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.1.0 numpy==1.26.0 scikit-learn==1.3.0 matplotlib==3.8.0 pandas==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14de3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import required libraries\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import copy\n",
    "# import os\n",
    "# import glob\n",
    "# from collections import deque\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# torch.manual_seed(7)\n",
    "# np.random.seed(7)\n",
    "\n",
    "# # Check for CUDA\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# General imports\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Allow importing from utils directory if needed (not strictly required in this notebook since we define code inline)\n",
    "# sys.path.append(\"utils/\")\n",
    "\n",
    "# Flag to use dummy data instead of real files (set False if real data is available)\n",
    "USE_DUMMY_DATA = False\n",
    "\n",
    "# Set a global random seed for reproducibility (if needed)\n",
    "np.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3db81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture: 1D CNN for N-BaIoT data\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_len):\n",
    "        \"\"\"CNN model for intrusion detection. output_len is the number of output classes.\"\"\"\n",
    "        super().__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=23, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.dropoutcv2 = nn.Dropout(p=0.3)   # Dropout after 3rd conv\n",
    "        self.conv4 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropoutcv3 = nn.Dropout(p=0.3)   # Dropout after 7th conv\n",
    "        self.conv8 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropoutcv4 = nn.Dropout(p=0.3)   # Dropout after 8th conv\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, output_len)\n",
    "        self.output_cnt = output_len  # number of output classes (used to handle binary vs multi-class output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass input through the convolutional layers with ReLU activations and occasional dropout\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropoutcv2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropoutcv3(x)\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropoutcv4(x)\n",
    "        # Flatten and pass through fully-connected layers\n",
    "        x = x.view(x.size(0), -1)   # flatten to [batch_size, features]\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        # For binary classification (output_len == 1), squeeze the output to a 1D tensor\n",
    "        if self.output_cnt == 1:\n",
    "            x = x.squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "def GetNbaIotModel(output_len):\n",
    "    \"\"\"Helper to instantiate the CNN model for N-BaIoT with given output length.\"\"\"\n",
    "    model = CNN(output_len)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb04944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllFeatureLabel(filenames, label_start_idx):\n",
    "    \"\"\"\n",
    "    Load data from multiple CSV files and return combined feature and label arrays.\n",
    "    Each file in 'filenames' is assumed to contain samples of a single class (with label included at column index label_start_idx).\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for idx, filepath in enumerate(filenames):\n",
    "        if USE_DUMMY_DATA:\n",
    "            # If using dummy data, generate random features and labels instead of reading files\n",
    "            sample_count = 100  # number of dummy samples per file (class)\n",
    "            feature_dim = label_start_idx  # number of feature columns\n",
    "            # Generate random feature values\n",
    "            dummy_features = np.random.rand(sample_count, feature_dim)\n",
    "            # Assign label = idx for all samples of this file (class index by order)\n",
    "            dummy_labels = np.full(shape=(sample_count,), fill_value=idx, dtype=float)\n",
    "            data_features = dummy_features\n",
    "            data_labels = dummy_labels\n",
    "        else:\n",
    "            # Load CSV data from file\n",
    "            # Assuming the CSV has no header and the label is at column 'label_start_idx'\n",
    "            data = np.loadtxt(filepath, delimiter=',', skiprows=0)\n",
    "            # Split features and label\n",
    "            data_features = data[:, :label_start_idx]\n",
    "            data_labels = data[:, label_start_idx:]\n",
    "            # If label column is one-hot or multi-column, flatten it\n",
    "            if data_labels.ndim > 1 and data_labels.shape[1] == 1:\n",
    "                data_labels = data_labels.flatten()\n",
    "        all_features.append(data_features)\n",
    "        all_labels.append(data_labels)\n",
    "    # Concatenate all files' data\n",
    "    all_features = np.vstack(all_features)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_features, all_labels\n",
    "\n",
    "def GetDataset(features, labels):\n",
    "    \"\"\"\n",
    "    Convert feature and label NumPy arrays into a PyTorch TensorDataset.\n",
    "    Features are cast to float32 tensors and labels to float (double) tensors.\n",
    "    \"\"\"\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels)  # labels will be float64 if labels array was float64\n",
    "    # Ensure labels tensor is 1D (flatten) if it's two-dimensional with a single column\n",
    "    if labels_tensor.ndim > 1:\n",
    "        labels_tensor = labels_tensor.view(-1)\n",
    "    return torch.utils.data.TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "def ShuffleDataset(dataset):\n",
    "    \"\"\"\n",
    "    Return a shuffled version of a TensorDataset.\n",
    "    \"\"\"\n",
    "    # Extract tensors from the dataset\n",
    "    features, labels = dataset[:]\n",
    "    # `dataset[:]` returns a tuple of (all_features_tensor, all_labels_tensor)\n",
    "    # Convert to NumPy for shuffling indices (or we can shuffle using torch directly)\n",
    "    indices = np.random.permutation(len(labels))\n",
    "    shuffled_features = features[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    return torch.utils.data.TensorDataset(shuffled_features, shuffled_labels)\n",
    "\n",
    "def SplitPrivateOpen(train_features, train_labels, private_percent, open_percent, class_count, random_selection):\n",
    "    \"\"\"\n",
    "    Split the combined training data of a device into private (labeled) data and open (unlabeled) data.\n",
    "    - private_percent: fraction of data to allocate to private (per class).\n",
    "    - open_percent: fraction to allocate to open (unlabeled) data.\n",
    "    - class_count: number of classes for this device.\n",
    "    - random_selection: if True, select open data randomly; if False, do stratified selection by taking the first portion of each class.\n",
    "    Returns: (private_features, private_labels, open_features, open_labels) as NumPy arrays.\n",
    "    \"\"\"\n",
    "    total_samples = len(train_labels)\n",
    "    private_features_list = []\n",
    "    private_labels_list = []\n",
    "    open_features_list = []\n",
    "    open_labels_list = []\n",
    "    # Compute split per class for stratification\n",
    "    for class_label in range(class_count):\n",
    "        # Indices for this class\n",
    "        class_indices = np.where(train_labels == class_label)[0]\n",
    "        if len(class_indices) == 0:\n",
    "            continue\n",
    "        class_size = len(class_indices)\n",
    "        open_count = int(class_size * open_percent)\n",
    "        if open_count < 1 and open_percent > 0:\n",
    "            open_count = 1  # ensure at least 1 sample if open_percent is >0 but class has few samples\n",
    "        # Shuffle class indices if random selection is enabled\n",
    "        if random_selection:\n",
    "            np.random.shuffle(class_indices)\n",
    "        # Split indices into private and open\n",
    "        open_idx = class_indices[:open_count]   # take first part as open\n",
    "        private_idx = class_indices[open_count:]  # rest as private\n",
    "        # Gather data for open\n",
    "        open_features_list.append(train_features[open_idx])\n",
    "        open_labels_list.append(train_labels[open_idx])\n",
    "        # Gather data for private\n",
    "        private_features_list.append(train_features[private_idx])\n",
    "        private_labels_list.append(train_labels[private_idx])\n",
    "    # Concatenate per-class splits back together\n",
    "    if len(private_features_list) > 0:\n",
    "        private_features = np.vstack(private_features_list)\n",
    "        private_labels = np.concatenate(private_labels_list)\n",
    "    else:\n",
    "        private_features = np.array([]).reshape(0, train_features.shape[1])\n",
    "        private_labels = np.array([])\n",
    "    if len(open_features_list) > 0:\n",
    "        open_features = np.vstack(open_features_list)\n",
    "        open_labels = np.concatenate(open_labels_list)\n",
    "    else:\n",
    "        open_features = np.array([]).reshape(0, train_features.shape[1])\n",
    "        open_labels = np.array([])\n",
    "    return private_features, private_labels, open_features, open_labels\n",
    "\n",
    "def SplitPrivate(features, labels, client_cnt, class_count, iid, data_average):\n",
    "    \"\"\"\n",
    "    Split a device's private data among its local clients.\n",
    "    - If iid is True: distribute data evenly and randomly among clients (IID split).\n",
    "    - If iid is False (non-IID 'equally' scenario): assign each class (or classes) to specific clients.\n",
    "    - data_average: if True, attempt to give each client equal number of samples (where possible).\n",
    "    Returns a list of TensorDataset objects, one per client.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    num_samples = len(labels)\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float64)\n",
    "    if iid:\n",
    "        # IID distribution: shuffle and partition roughly equally\n",
    "        indices = torch.randperm(num_samples)\n",
    "        # Split indices into `client_cnt` chunks\n",
    "        splits = torch.chunk(indices, client_cnt)\n",
    "        for split in splits:\n",
    "            split_feat = features[split]\n",
    "            split_label = labels[split]\n",
    "            datasets.append(torch.utils.data.TensorDataset(split_feat, split_label))\n",
    "    else:\n",
    "        # Non-IID 'equally': try to allocate one class per client (or each class to a subset of clients)\n",
    "        # Determine how many clients per class if classes < clients\n",
    "        if class_count <= client_cnt:\n",
    "            # Number of clients each class should occupy (at least floor, some classes may occupy an extra client if not perfectly divisible)\n",
    "            base_clients_per_class = client_cnt // class_count\n",
    "            extra = client_cnt % class_count\n",
    "        else:\n",
    "            # More classes than clients (not expected in our setup); in this case, some clients will handle multiple classes\n",
    "            base_clients_per_class = 1\n",
    "            extra = 0\n",
    "        client_allocations = [[] for _ in range(client_cnt)]\n",
    "        current_client = 0\n",
    "        # Assign each class's data to one or more clients\n",
    "        for class_label in range(class_count):\n",
    "            class_indices = (labels == float(class_label)).nonzero(as_tuple=True)[0]\n",
    "            if len(class_indices) == 0:\n",
    "                continue\n",
    "            # Determine number of clients to assign this class\n",
    "            k = base_clients_per_class + (1 if class_label < extra else 0)\n",
    "            if k < 1: \n",
    "                k = 1\n",
    "            # Split class samples into k parts (as equal as possible) for k clients\n",
    "            if k == 1:\n",
    "                # All samples of this class go to one client\n",
    "                client_allocations[current_client].extend(class_indices.tolist())\n",
    "                current_client = (current_client + 1) % client_cnt\n",
    "            else:\n",
    "                # Shuffle class indices for fair distribution\n",
    "                perm = class_indices[torch.randperm(len(class_indices))]\n",
    "                parts = torch.chunk(perm, k)\n",
    "                for part in parts:\n",
    "                    client_allocations[current_client].extend(part.tolist())\n",
    "                    current_client = (current_client + 1) % client_cnt\n",
    "        # Now create dataset for each client from allocated indices\n",
    "        for alloc in client_allocations:\n",
    "            if len(alloc) == 0:\n",
    "                # If a client got no data, give an empty dataset\n",
    "                datasets.append(torch.utils.data.TensorDataset(torch.empty((0, features.shape[1])), torch.empty((0,), dtype=torch.float64)))\n",
    "            else:\n",
    "                alloc_idx = torch.tensor(alloc, dtype=torch.long)\n",
    "                datasets.append(torch.utils.data.TensorDataset(features[alloc_idx], labels[alloc_idx]))\n",
    "    return datasets\n",
    "\n",
    "def DilSplitPrivate(features, labels, client_cnt, class_count, alpha, seed):\n",
    "    \"\"\"\n",
    "    Split a device's private data among clients using a Dirichlet distribution (non-IID).\n",
    "    Each class's samples are divided among clients according to a Dirichlet(alpha) random proportion.\n",
    "    - alpha: Dirichlet concentration parameter (smaller => more skewed/non-iid distribution).\n",
    "    - seed: random seed for reproducibility.\n",
    "    Returns a list of TensorDataset objects for each client.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(int(seed))\n",
    "    # Initialize empty index list for each client\n",
    "    client_indices = [[] for _ in range(client_cnt)]\n",
    "    # Convert to NumPy for easier grouping\n",
    "    labels_np = labels if isinstance(labels, np.ndarray) else np.array(labels)\n",
    "    # For each class, allocate its samples to clients based on Dirichlet draw\n",
    "    for class_label in range(class_count):\n",
    "        class_indices = np.where(labels_np == class_label)[0]\n",
    "        if len(class_indices) == 0:\n",
    "            continue\n",
    "        # Draw random proportions for this class among clients\n",
    "        proportions = np.random.dirichlet([alpha] * client_cnt)\n",
    "        # Number of samples per client for this class (round down)\n",
    "        class_counts = (proportions * len(class_indices)).astype(int)\n",
    "        # Adjust counts to ensure total equals len(class_indices)\n",
    "        diff = len(class_indices) - class_counts.sum()\n",
    "        # Distribute any rounding difference\n",
    "        for i in range(diff):\n",
    "            # assign one extra sample to clients with largest remaining proportion\n",
    "            class_counts[np.argmax(proportions)] += 1\n",
    "        # Shuffle class indices and split according to counts\n",
    "        np.random.shuffle(class_indices)\n",
    "        start = 0\n",
    "        for client_id, count in enumerate(class_counts):\n",
    "            if count > 0:\n",
    "                subset = class_indices[start : start + count]\n",
    "                client_indices[client_id].extend(subset.tolist())\n",
    "                start += count\n",
    "    # Create TensorDataset for each client's indices\n",
    "    datasets = []\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float64)\n",
    "    for idx_list in client_indices:\n",
    "        if len(idx_list) == 0:\n",
    "            datasets.append(torch.utils.data.TensorDataset(torch.empty((0, features_tensor.shape[1])), torch.empty((0,), dtype=torch.float64)))\n",
    "        else:\n",
    "            idx_tensor = torch.tensor(idx_list, dtype=torch.long)\n",
    "            datasets.append(torch.utils.data.TensorDataset(features_tensor[idx_tensor], labels_tensor[idx_tensor]))\n",
    "    return datasets\n",
    "\n",
    "def GetFeatureFromOpenDataset(open_dataset, start_idx, end_idx):\n",
    "    \"\"\"\n",
    "    Extract a slice [start_idx:end_idx] from the open (unlabeled) dataset.\n",
    "    Used to obtain a batch of open data for processing in each communication round.\n",
    "    Returns (feature_tensor, label_tensor) for that slice (labels are just dummy ground-truth labels, since open data is unlabeled).\n",
    "    \"\"\"\n",
    "    total = len(open_dataset)\n",
    "    if end_idx > total:\n",
    "        end_idx = total\n",
    "    # open_dataset[:] gives (features_tensor, labels_tensor) of the whole dataset\n",
    "    features_tensor, labels_tensor = open_dataset[:]\n",
    "    # Slice the tensors\n",
    "    feature_slice = features_tensor[start_idx:end_idx]\n",
    "    label_slice = labels_tensor[start_idx:end_idx]\n",
    "    return feature_slice, label_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d52f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(dev, feature, model, model_out_len):\n",
    "    \"\"\"\n",
    "    Generate hard predictions (labels) from the model for the given features.\n",
    "    Returns a tensor of predicted labels.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model = model.to(dev)\n",
    "        feature = feature.to(dev)\n",
    "        logits = model(feature)\n",
    "        pred_label = Logits2PredLabel(logits, model_out_len)\n",
    "        return pred_label\n",
    "\n",
    "def Logits2PredLabel(logits, model_out_len):\n",
    "    \"\"\"Convert raw model outputs (logits) to hard labels.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if model_out_len == 1:\n",
    "            # Binary classification: apply sigmoid and threshold at 0.5 (round)\n",
    "            prediction = torch.round(torch.sigmoid(logits))\n",
    "        else:\n",
    "            # Multi-class: take the index of the max logit as the predicted class\n",
    "            _, prediction = torch.max(logits, dim=1)\n",
    "        return prediction\n",
    "\n",
    "def Predict2SoftLabel(dev, feature, model, model_out_len):\n",
    "    \"\"\"\n",
    "    Generate soft label predictions (probability distribution or logits) from the model.\n",
    "    Returns a tensor of soft labels (probabilities for each class).\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model = model.to(dev)\n",
    "        feature = feature.to(dev)\n",
    "        logits = model(feature)\n",
    "        soft_logits = Logits2Soft(logits, model_out_len)\n",
    "        return soft_logits\n",
    "\n",
    "def Logits2Soft(logits, model_out_len):\n",
    "    \"\"\"\n",
    "    Convert logits to a normalized probability distribution (soft labels).\n",
    "    For binary, produce a 2-dimensional probability (for class 0 and class 1).\n",
    "    For multi-class, apply softmax.\n",
    "    \"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        if model_out_len == 1:\n",
    "            # Binary case: apply sigmoid to get probability of class \"1\"\n",
    "            logits = sigmoid(logits)\n",
    "            # Construct 2-column softmax-like output [P(class0), P(class1)] for each sample\n",
    "            soft_max_logits = torch.zeros(len(logits), 2)\n",
    "            for i in range(len(logits)):\n",
    "                soft_max_logits[i] = torch.tensor([1 - logits[i].item(), logits[i].item()])\n",
    "            probabilities = soft_max_logits\n",
    "        else:\n",
    "            # Multi-class: softmax to get probability distribution over classes\n",
    "            probabilities = softmax(logits)\n",
    "        return probabilities\n",
    "\n",
    "def HardLabel(soft_label):\n",
    "    \"\"\"\n",
    "    Convert a soft label (probability vector) to a hard label index or 'unknown'.\n",
    "    If the highest probability is > 1/num_classes, returns that class index; \n",
    "    otherwise returns 'class_cat' (an index used to denote uncertain/unknown).\n",
    "    \"\"\"\n",
    "    sample_cnt = len(soft_label)\n",
    "    class_cat = len(soft_label[0])  # number of classes\n",
    "    boundary = 1.0 / class_cat\n",
    "    hard_label = [0] * sample_cnt\n",
    "    for i in range(sample_cnt):\n",
    "        cur_soft = soft_label[i]\n",
    "        pred_label = torch.argmax(cur_soft).item()\n",
    "        pred_proba = torch.max(cur_soft).item()\n",
    "        # If the highest probability is greater than the uniform probability threshold, accept it; otherwise label as 'unknown' (class_cat index)\n",
    "        hard_label[i] = pred_label if pred_proba > boundary else class_cat\n",
    "    return hard_label\n",
    "\n",
    "def HardLabelVoteHard(all_client_hard_label, class_cat):\n",
    "    \"\"\"\n",
    "    Perform majority vote across clients for each sample's hard label.\n",
    "    all_client_hard_label: list of hard label lists from each client.\n",
    "    class_cat: number of normal classes (for multi-class, equals number of classes; an extra index is used to denote 'unknown').\n",
    "    Returns a tensor of voted labels for each sample.\n",
    "    \"\"\"\n",
    "    client_cnt = len(all_client_hard_label)\n",
    "    sample_cnt = len(all_client_hard_label[0])\n",
    "    voted_labels = []\n",
    "    for i in range(sample_cnt):\n",
    "        # Tally votes for each class (not counting 'unknown' votes which are denoted by class_cat index)\n",
    "        votes = [0] * class_cat\n",
    "        for j in range(client_cnt):\n",
    "            lbl = all_client_hard_label[j][i]\n",
    "            if lbl != class_cat:  # ignore 'unknown' votes\n",
    "                votes[int(lbl)] += 1\n",
    "        # Determine the class with maximum votes (default 0 if all votes are 'unknown')\n",
    "        if max(votes) == 0:\n",
    "            # No client had a confident label (all marked unknown)\n",
    "            voted_labels.append(class_cat)  # mark as unknown\n",
    "        else:\n",
    "            voted_labels.append(int(np.argmax(votes)))\n",
    "    voted_labels = torch.tensor(voted_labels)\n",
    "    return voted_labels\n",
    "\n",
    "def HardLabelVoteOneHot(all_client_hard_label, class_cat):\n",
    "    \"\"\"\n",
    "    Similar to HardLabelVoteHard, but returns one-hot encoded vectors of the voted label for each sample.\n",
    "    \"\"\"\n",
    "    # Get hard voted label per sample\n",
    "    hard_votes = HardLabelVoteHard(all_client_hard_label, class_cat)\n",
    "    # Convert to one-hot representation\n",
    "    one_hot_results = []\n",
    "    for label in hard_votes:\n",
    "        one_hot = [0.0] * class_cat\n",
    "        if label < class_cat:\n",
    "            one_hot[int(label)] = 1.0\n",
    "        # if label == class_cat (unknown), one-hot will remain all zeros (or conceptually an 'unknown' category)\n",
    "        one_hot_results.append(one_hot)\n",
    "    one_hot_tensor = torch.tensor(one_hot_results)\n",
    "    return one_hot_tensor\n",
    "\n",
    "def OneHot2Label(one_hot_vectors):\n",
    "    \"\"\"\n",
    "    Convert one-hot encoded label vectors back to class indices.\n",
    "    \"\"\"\n",
    "    _, labels = torch.max(one_hot_vectors, dim=1)\n",
    "    labels = labels.double()  # convert to double for consistency with other labels in this codebase\n",
    "    return labels\n",
    "\n",
    "def PredictAvg(dev, dataset, bounds, model):\n",
    "    \"\"\"\n",
    "    Compute the average prediction (softmax output) for each class from a dataset slice.\n",
    "    Not used in main training; possibly for analysis.\n",
    "    - bounds: list of (start_idx, end_idx) for each class's range in the dataset.\n",
    "    Returns a dictionary mapping class label -> average probability vector for that class.\n",
    "    \"\"\"\n",
    "    print(\"\\npred avg\")\n",
    "    print(bounds)\n",
    "    each_label_avg_logit = {}\n",
    "    soft_max = torch.nn.Softmax(dim=1)\n",
    "    model = model.to(dev)\n",
    "    for i in range(len(bounds)):\n",
    "        start_idx, end_idx = bounds[i]\n",
    "        if start_idx == end_idx:\n",
    "            continue  # no samples of this class\n",
    "        # Extract features and labels for this class range\n",
    "        class_features, class_labels = dataset[start_idx:end_idx]\n",
    "        with torch.no_grad():\n",
    "            class_features = class_features.to(dev)\n",
    "            preds = model(class_features)\n",
    "            preds = soft_max(preds)\n",
    "            mean_pred = torch.mean(preds, dim=0)  # average probability vector\n",
    "            true_label = class_labels[0].item() if len(class_labels) > 0 else i\n",
    "            each_label_avg_logit[int(true_label)] = mean_pred.detach().clone()\n",
    "    print(\"end pred avg\")\n",
    "    return each_label_avg_logit\n",
    "\n",
    "def PredictFilter(dev, open_feature, classify_model, classify_model_len_out_tensor, class_cat, theta):\n",
    "    \"\"\"\n",
    "    Filter out uncertain predictions from a model on open data given a threshold theta.\n",
    "    If model's confidence (max probability) for a sample is below theta, that sample's soft label is replaced with a uniform distribution (treated as unknown).\n",
    "    - theta: threshold for confidence. If theta < 0, it's dynamically set as median of max probabilities.\n",
    "    \"\"\"\n",
    "    print(\"\\nin predict filter\")\n",
    "    print(f\"theta = {theta}\")\n",
    "    classify_model = classify_model.to(dev)\n",
    "    average_tensor = torch.tensor([1.0 / class_cat] * class_cat)  # uniform distribution vector\n",
    "    with torch.no_grad():\n",
    "        open_feature = open_feature.to(dev)\n",
    "        soft_labels = Logits2Soft(classify_model(open_feature), classify_model_len_out_tensor)\n",
    "        if theta < 0:\n",
    "            # if theta is negative, use median of confidence scores as threshold\n",
    "            max_vals, _ = torch.max(soft_labels, 1)\n",
    "            theta = max_vals.median()\n",
    "        # Replace any soft label whose max probability is below theta with a uniform distribution (unknown)\n",
    "        for i in range(len(soft_labels)):\n",
    "            max_proba = torch.max(soft_labels[i])\n",
    "            if max_proba < theta:\n",
    "                soft_labels[i] = average_tensor.clone()\n",
    "    return soft_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b24247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithFeatureLabel(dev, feature, label, batchsize, model, opt, loss_func):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on the given feature & label tensors (treated as a dataset).\n",
    "    Returns the average loss over the dataset.\n",
    "    \"\"\"\n",
    "    # Create a TensorDataset on-the-fly from the features and labels\n",
    "    dataset = torch.utils.data.TensorDataset(feature, label)\n",
    "    avg_loss = TrainWithDataset(dev, dataset, batchsize, model, opt, loss_func)\n",
    "    return avg_loss\n",
    "\n",
    "def TrainWithDataset(dev, dataset, batchsize, model, opt, loss_func):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on the given dataset (torch.utils.data.TensorDataset).\n",
    "    Uses given optimizer and loss function. Returns the average loss.\n",
    "    \"\"\"\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    model = model.to(dev)\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_feature, batch_label in data_loader:\n",
    "        opt.zero_grad()\n",
    "        batch_feature = batch_feature.to(dev)\n",
    "        batch_label = batch_label.to(dev)\n",
    "        preds = model(batch_feature)\n",
    "        loss = loss_func(preds, batch_label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * batch_label.size(0)\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def EvalWithFeatureLabel(dev, feature, label, batchsize, model, loss_func):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given feature & label tensors. Returns the average loss.\n",
    "    \"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(feature, label)\n",
    "    avg_loss = EvalWithDataset(dev, dataset, batchsize, model, loss_func)\n",
    "    return avg_loss\n",
    "\n",
    "def EvalWithDataset(dev, dataset, batchsize, model, loss_func):\n",
    "    \"\"\"\n",
    "    Compute the loss of the model on the entire dataset (without updating weights).\n",
    "    \"\"\"\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test_loader:\n",
    "            feature = feature.to(dev)\n",
    "            label = label.to(dev)\n",
    "            out = model(feature)\n",
    "            loss = loss_func(out, label)\n",
    "            total_loss += loss.item() * label.size(0)\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def Metrics(true_label, pred_label):\n",
    "    \"\"\"\n",
    "    Compute simple accuracy given true labels and predicted labels.\n",
    "    Returns (correct_count, accuracy).\n",
    "    Also prints an 'Evaluation...' message (as in original code).\n",
    "    \"\"\"\n",
    "    print(\"Evaluation...\")\n",
    "    all_preds = pred_label.cpu()\n",
    "    all_labels = true_label.cpu()\n",
    "    correct_num = (all_labels == all_preds).sum().item()\n",
    "    test_acc = correct_num / len(true_label)\n",
    "    return correct_num, test_acc\n",
    "\n",
    "def PredictWithDisUnknown(dev, open_feature, classify_model, classify_model_len_out_tensor, discri_model, discri_model_len_out_tensor, class_cat):\n",
    "    \"\"\"\n",
    "    Use both classifier and discriminator to predict labels for open_feature.\n",
    "    - classify_model produces soft labels (probabilities) for classes.\n",
    "    - discri_model predicts which samples are 'unknown' (for binary discriminator, output 1 means unknown).\n",
    "    For each sample: if discriminator predicts it as unknown (1), replace its soft label with a uniform 'average' vector.\n",
    "    Returns a tensor of soft labels for the open_feature (with unknowns adjusted).\n",
    "    \"\"\"\n",
    "    discri_model = discri_model.to(dev)\n",
    "    classify_model = classify_model.to(dev)\n",
    "    # A uniform average probability vector for 'unknown' cases\n",
    "    average_tensor = torch.tensor([1.0 / class_cat] * class_cat)\n",
    "    with torch.no_grad():\n",
    "        open_feature = open_feature.to(dev)\n",
    "        # Get soft class probabilities from the classifier\n",
    "        class_logits = classify_model(open_feature)\n",
    "        soft_labels = Logits2Soft(class_logits, classify_model_len_out_tensor)\n",
    "        # Get discriminator predictions (hard labels: 0 for known, 1 for unknown if binary output)\n",
    "        dis_logits = discri_model(open_feature)\n",
    "        dis_pred = Logits2PredLabel(dis_logits, discri_model_len_out_tensor)\n",
    "        # For each sample, if discriminator says \"unknown\", replace its soft label with uniform distribution\n",
    "        for i in range(len(dis_pred)):\n",
    "            if dis_pred[i].item() == 1:  # 1 indicates the sample is predicted as 'unknown' by discriminator\n",
    "                soft_labels[i] = average_tensor.clone()\n",
    "    return soft_labels\n",
    "\n",
    "def GetDeviceClientCnt(device_name, client_cnt, classify_model_out_len):\n",
    "    \"\"\"\n",
    "    Determine how many clients to instantiate for a given device.\n",
    "    If binary classification, return 4 (we limit to 4 clients per device).\n",
    "    If multi-class:\n",
    "      - For certain devices (Ennio Doorbell, Samsung Webcam), use roughly half the number of clients (since they have fewer attack types).\n",
    "      - Otherwise, use the default client_cnt from config.\n",
    "    \"\"\"\n",
    "    if classify_model_out_len == 1:\n",
    "        return 4\n",
    "    else:\n",
    "        if device_name in [\"Ennio_Doorbell/\", \"Samsung_SNH_1011_N_Webcam/\"]:\n",
    "            return int(client_cnt / 2) + 1\n",
    "        else:\n",
    "            return client_cnt\n",
    "\n",
    "def GetDeviceClassCat(device_name, classify_model_out_len):\n",
    "    \"\"\"\n",
    "    Determine the number of classes ('class_cat') for a given device.\n",
    "    If binary (output_len==1): 2 classes (benign vs attack).\n",
    "    If multi-class:\n",
    "      - For Ennio Doorbell and Samsung Webcam devices, return 6 (these devices have only 5 attacks + benign in the dataset).\n",
    "      - Otherwise, return 11 (all 10 attacks + benign).\n",
    "    \"\"\"\n",
    "    if classify_model_out_len == 1:\n",
    "        return 2\n",
    "    if device_name in [\"Ennio_Doorbell/\", \"Samsung_SNH_1011_N_Webcam/\"]:\n",
    "        return 6\n",
    "    else:\n",
    "        return 11\n",
    "\n",
    "def reshape_sample(feature):\n",
    "    \"\"\"\n",
    "    Reshape the feature matrix into shape (-1, 23, 5).\n",
    "    This organizes 115 features into a 23x5 matrix for CNN input.\n",
    "    \"\"\"\n",
    "    # -1 infers the batch dimension, 23 channels, 5 timesteps\n",
    "    reshaped = np.reshape(feature, (-1, 23, 5))\n",
    "    return reshaped\n",
    "\n",
    "def PredUnknown(dev, feature, model, theta, model_out_len):\n",
    "    \"\"\"\n",
    "    Identify 'unknown' samples from a feature batch using the given model (classifier).\n",
    "    - It passes the features through the model and obtains probabilities.\n",
    "    - If model_out_len == 1 (binary classifier for known vs unknown), use sigmoid to get probability of 'attack'.\n",
    "      Then construct a 2-class probability [P(known), P(unknown)] for each sample.\n",
    "    - If multi-class, uses softmax to get class probabilities.\n",
    "    - Then finds samples where the highest probability is below threshold theta (meaning the model is not confident in any known class).\n",
    "    Returns a tensor of features that are considered 'sure unknown'. If none found, returns None.\n",
    "    \"\"\"\n",
    "    sure_unknown = []\n",
    "    wait_to_distill = []\n",
    "    soft_max = torch.nn.Softmax(dim=1)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    model = model.to(dev)\n",
    "    feature = feature.to(dev)\n",
    "    with torch.no_grad():\n",
    "        out = model(feature)\n",
    "        if model_out_len == 1:\n",
    "            # Binary case: model output is logit for \"attack\", compute probability and form 2-class output\n",
    "            out = sigmoid(out)\n",
    "            prob2 = torch.zeros(len(out), 2)\n",
    "            for i in range(len(out)):\n",
    "                prob2[i] = torch.tensor([1 - out[i].item(), out[i].item()])\n",
    "            out = prob2\n",
    "        else:\n",
    "            out = soft_max(out)\n",
    "        # Determine threshold if theta is dynamic\n",
    "        if theta < 0:\n",
    "            theta = torch.median(torch.max(out, dim=1).values)\n",
    "        # Separate features into 'unknown' and 'confident' sets based on threshold\n",
    "        max_vals, pred_labels = torch.max(out, 1)\n",
    "        for i in range(len(max_vals)):\n",
    "            if max_vals[i] < theta:\n",
    "                sure_unknown.append(feature[i])\n",
    "            else:\n",
    "                wait_to_distill.append(feature[i])\n",
    "        if len(sure_unknown) == 0:\n",
    "            return None\n",
    "        sure_unknown = torch.stack(sure_unknown)\n",
    "    return sure_unknown\n",
    "\n",
    "def LabelFeature(feature, label):\n",
    "    \"\"\"\n",
    "    Create a label tensor for all samples in 'feature', filled with the given label value.\n",
    "    Returns a tuple (feature, labels_tensor).\n",
    "    \"\"\"\n",
    "    labels = torch.tensor([label] * len(feature), dtype=torch.double)\n",
    "    return feature, labels\n",
    "\n",
    "def DisUnknown(dev, client, dis_rounds, batchsize, dis_train_feature, theta):\n",
    "    \"\"\"\n",
    "    Train a client's discriminator model to distinguish 'unknown' samples from known ones.\n",
    "    Process:\n",
    "      - Identify 'sure unknown' samples from the provided open data using client's classifier (PredUnknown).\n",
    "      - Label all 'sure unknown' samples with unknown_label (1) and all client's known private samples as known_label (0).\n",
    "      - Combine these into a training set for the discriminator.\n",
    "      - Train the discriminator for 'dis_rounds' epochs on this dataset.\n",
    "    Returns True if training succeeded (unknown samples found), or False if no unknown samples were identified.\n",
    "    \"\"\"\n",
    "    # Make a copy of the feature data to avoid modifying original\n",
    "    dis_feature_pool = dis_train_feature.detach().clone()\n",
    "    # Use the client's classifier to predict which open samples are definitely unknown\n",
    "    sure_unknown_feature = PredUnknown(dev, dis_feature_pool, client.classify_model, theta, client.classify_model_out_len)\n",
    "    if sure_unknown_feature is None:\n",
    "        # No unknown samples detected, hence skip discriminator training\n",
    "        return False\n",
    "    # Determine labels for discriminator: unknown samples = 1, known samples = 0\n",
    "    if client.discri_model_out_len == 1:\n",
    "        unknown_label_num = 1.0\n",
    "        known_label_num = 0.0\n",
    "    else:\n",
    "        unknown_label_num = 1\n",
    "        known_label_num = 0\n",
    "    # Create labeled datasets for unknown and known\n",
    "    unknown_feat, unknown_lbl = LabelFeature(sure_unknown_feature, unknown_label_num)\n",
    "    known_feat, _ = client.classify_dataset[:]  # all known private data features\n",
    "    known_feat = known_feat.detach().clone()\n",
    "    known_feat, known_lbl = LabelFeature(known_feat, known_label_num)\n",
    "    # Combine known and unknown samples for discriminator training\n",
    "    combined_features = torch.cat((known_feat.to(dev), unknown_feat.to(dev)), dim=0)\n",
    "    combined_labels = torch.cat((known_lbl.to(dev), unknown_lbl.to(dev)), dim=0)\n",
    "    # To avoid GPU memory issues during assembly, move combined to CPU for shuffling\n",
    "    combined_features = combined_features.to(torch.device(\"cpu\"))\n",
    "    combined_labels = combined_labels.to(torch.device(\"cpu\"))\n",
    "    dis_dataset = torch.utils.data.TensorDataset(combined_features, combined_labels)\n",
    "    # Shuffle the combined dataset before training\n",
    "    dis_dataset = ShuffleDataset(dis_dataset)\n",
    "    # Train the discriminator model for the specified number of rounds (epochs)\n",
    "    for r in range(dis_rounds):\n",
    "        TrainWithDataset(dev, dis_dataset, batchsize, client.discri_model, client.discri_opt, client.discri_loss_func)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e30f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSFL_IDS(conf, dev, clients, server, test_dataset, open_dataset):\n",
    "    \"\"\"\n",
    "    Core Semi-Supervised Federated Learning (SSFL-IDS) algorithm.\n",
    "    - conf: dictionary of configuration parameters.\n",
    "    - dev: torch.device (CPU or CUDA).\n",
    "    - clients: list of client objects.\n",
    "    - server: server object.\n",
    "    - test_dataset: global test dataset (for evaluation).\n",
    "    - open_dataset: the unlabeled open dataset accessible to all clients.\n",
    "    \n",
    "    The training proceeds in communication rounds. Each round consists of:\n",
    "      Stage I: Supervised client training on private data + Unknown detection training.\n",
    "      Stage II: Knowledge distillation on open data (clients and server).\n",
    "    \"\"\"\n",
    "    comm_cnt = conf[\"comm_cnt\"]                # total number of communication rounds\n",
    "    open_idx_set_cnt = conf[\"open_idx_set_cnt\"]  # number of open samples to use each round\n",
    "    batchsize = conf[\"batchsize\"]\n",
    "    train_rounds = conf[\"train_rounds\"]        # epochs of classifier training per round (after first)\n",
    "    dis_rounds = conf[\"discri_rounds\"]         # epochs for discriminator training\n",
    "    dist_rounds = conf[\"dist_rounds\"]          # epochs for distillation training\n",
    "    theta = conf[\"theta\"]                      # threshold for unknown detection (if -1, dynamic thresholding)\n",
    "    labels = conf[\"labels\"]                    # list of class labels (not used directly in code logic)\n",
    "    first_train_rounds = conf[\"first_train_rounds\"]  # extra epochs of classifier training in first round\n",
    "    class_cat = conf[\"classify_model_out_len\"] if conf[\"classify_model_out_len\"] > 1 else 2\n",
    "    dis_train_cnt = 10000  # number of open samples to use for discriminator training each round (hardcoded)\n",
    "    start_idx = 0\n",
    "    end_idx = start_idx + open_idx_set_cnt\n",
    "    open_len = len(open_dataset)\n",
    "    \n",
    "    for e in range(comm_cnt):\n",
    "        sure_unknown_none = set()   # set of client indices that found no unknown samples\n",
    "        all_client_hard_label = []\n",
    "        # --- Stage I: Clients training and labeling ---\n",
    "        print(f\"Round {e+1} Stage I\")\n",
    "        client_cnt = len(clients)\n",
    "        participate = 0  # count how many clients actually participated (had non-zero private data)\n",
    "        # Slice a batch of open data for this round (or entire open data if smaller)\n",
    "        open_feature, open_label = GetFeatureFromOpenDataset(open_dataset, start_idx, end_idx)\n",
    "        # If the requested open batch is larger than the available open set, adjust global_logits size\n",
    "        if open_idx_set_cnt > open_len:\n",
    "            global_logits = torch.zeros(open_len, len(labels))\n",
    "        else:\n",
    "            global_logits = torch.zeros(open_idx_set_cnt, len(labels))\n",
    "        # Each client performs local training on its labeled data and uses its discriminator to label open data\n",
    "        for c_idx, client in enumerate(clients):\n",
    "            print(f\"Client {c_idx+1} Training...\")\n",
    "            # Determine how many epochs to train classifier this round\n",
    "            cur_train_rounds = train_rounds if e != 0 else first_train_rounds\n",
    "            # If a client has no private data (just in case), skip it\n",
    "            if len(client.classify_dataset) == 0:\n",
    "                continue\n",
    "            # Supervised training on client's labeled private dataset\n",
    "            for _ in range(cur_train_rounds):\n",
    "                TrainWithDataset(dev, client.classify_dataset, batchsize, client.classify_model, client.classify_opt, client.hard_label_loss_func)\n",
    "            # Mark this client as having participated (has labeled data)\n",
    "            if sum(count > 0 for count in client.each_class_cnt) > 0:\n",
    "                participate += 1\n",
    "            # Prepare data for discriminator training (we use a fixed number of open samples dis_train_cnt for unknown detection)\n",
    "            dis_train_feature, _ = GetFeatureFromOpenDataset(open_dataset, 0, dis_train_cnt)\n",
    "            # Train client's discriminator to identify unknown samples (DisUnknown performs this training)\n",
    "            success = DisUnknown(dev, client, dis_rounds, batchsize, dis_train_feature, theta)\n",
    "            if not success:\n",
    "                # If no unknown found, note this client (its predictions will be trusted fully, no unknown filtering)\n",
    "                sure_unknown_none.add(c_idx)\n",
    "            # Now use client's classifier & (updated) discriminator to predict labels for the current open_feature batch\n",
    "            client_open_feature = open_feature.detach().clone()  # copy open feature tensor\n",
    "            if c_idx not in sure_unknown_none:\n",
    "                # If the client has a working discriminator (found unknowns), filter unknowns\n",
    "                local_soft = PredictWithDisUnknown(dev, client_open_feature, client.classify_model, client.classify_model_out_len, client.discri_model, client.discri_model_out_len, len(labels))\n",
    "            else:\n",
    "                # If no unknowns were found by this client, just use classifier's soft predictions (no filtering)\n",
    "                local_soft = Predict2SoftLabel(dev, client_open_feature, client.classify_model, client.classify_model_out_len)\n",
    "            # Convert the client's soft predictions to hard labels (with 'unknown' possibility)\n",
    "            hard_label = HardLabel(local_soft)\n",
    "            all_client_hard_label.append(hard_label)\n",
    "            print()  # newline for readability\n",
    "        \n",
    "        # Perform majority vote across all clients' hard labels for the open batch\n",
    "        global_hard_labels = HardLabelVoteHard(all_client_hard_label, class_cat)\n",
    "        # Convert the voted hard labels to one-hot (for distillation training if needed)\n",
    "        global_logits = HardLabelVoteOneHot(all_client_hard_label, class_cat)\n",
    "        # --- Stage II: Distillation training ---\n",
    "        print(f\"Round {e+1} Stage II\")\n",
    "        # Each client performs distillation training on the open data with the global pseudo-labels\n",
    "        for c_idx, client in enumerate(clients):\n",
    "            print(f\"Client {c_idx+1} Distillation Training...\")\n",
    "            for _ in range(dist_rounds):\n",
    "                # Use global one-hot labels as targets for client's classifier on open_feature\n",
    "                # If classifier output is binary, convert one-hot targets to 0/1 labels\n",
    "                if client.classify_model_out_len != 1:\n",
    "                    TrainWithFeatureLabel(dev, open_feature.detach().clone(), global_logits.detach().clone(), batchsize, client.classify_model, client.classify_opt, client.hard_label_loss_func)\n",
    "                else:\n",
    "                    # For binary classifier, convert one-hot global logits to label 0/1\n",
    "                    binary_targets = OneHot2Label(global_logits)\n",
    "                    TrainWithFeatureLabel(dev, open_feature.detach().clone(), binary_targets, batchsize, client.classify_model, client.classify_opt, client.hard_label_loss_func)\n",
    "        # The server also performs distillation on the open data using the aggregated labels (the server's model learns from the consensus of clients)\n",
    "        print(\"Server Training...\")\n",
    "        for _ in range(dist_rounds):\n",
    "            if server.model_out_len != 1:\n",
    "                TrainWithFeatureLabel(dev, open_feature.detach().clone(), global_logits.detach().clone(), batchsize, server.model, server.dist_opt, server.hard_label_loss_func)\n",
    "            else:\n",
    "                binary_targets = OneHot2Label(global_logits)\n",
    "                TrainWithFeatureLabel(dev, open_feature.detach().clone(), binary_targets, batchsize, server.model, server.dist_opt, server.hard_label_loss_func)\n",
    "        # Evaluate the server's global model on the test dataset and report accuracy\n",
    "        test_feature, test_label = test_dataset[:]\n",
    "        pred_label = Predict(dev, test_feature, server.model, server.model_out_len)\n",
    "        _, test_acc = Metrics(test_label, pred_label)\n",
    "        print(f\"Round {e+1} Test Acc = {test_acc} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f63f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSFL_IDS_NBaIoT():\n",
    "    \"\"\"\n",
    "    Setup and execute SSFL-IDS training for the N-BaIoT dataset scenario.\n",
    "    This function initializes the dataset, clients, and server, then calls SSFL_IDS().\n",
    "    \"\"\"\n",
    "    # Configuration parameters for N-BaIoT experiment\n",
    "    configs = {\n",
    "        \"comm_cnt\": 201,             # total communication rounds\n",
    "        \"device_client_cnt\": 11,     # base number of clients per device\n",
    "        \"private_percent\": 0.9,      # 90% of each device's data is private (labeled)\n",
    "        \"batchsize\": 100,\n",
    "        \"iid\": False,               # data distribution is non-IID among clients\n",
    "        \"need_dist\": True,          # whether distillation is needed (True in our method)\n",
    "        \"open_percent\": 0.1,        # 10% of data is open (unlabeled)\n",
    "        \"label_lr\": 0.0001,         # learning rate for classifier models\n",
    "        \"dist_lr\": 0.0001,          # learning rate for server model (distillation)\n",
    "        \"discri_lr\": 0.0001,        # learning rate for discriminator models\n",
    "        \"train_rounds\": 3,          # epochs of classifier training each round (after first)\n",
    "        \"discri_rounds\": 3,         # epochs of discriminator training each round\n",
    "        \"dist_rounds\": 10,          # epochs of distillation training each round\n",
    "        \"first_train_rounds\": 3,    # epochs of classifier training in round 1 (may be higher to prime the model)\n",
    "        \"open_idx_set_cnt\": 10000,  # number of open-set samples to use in each round for label aggregation\n",
    "        \"discri_cnt\": 10000,        # (not explicitly used above, possibly same as open_idx_set_cnt for discrimination)\n",
    "        \"dist_T\": 0.1,              # temperature for distillation (not explicitly used in this code, possibly for an alternate approach)\n",
    "        \"need_SA\": False,           # (possibly for another method like spectral augmentation, not used here)\n",
    "        \"test_batch_size\": 256,\n",
    "        \"label_start_idx\": 115,     # index where label starts in the CSV (features are 0-114)\n",
    "        \"test_round\": 1,            # possibly how often to test (not used in code, we test every round)\n",
    "        \"data_average\": True,       # whether to equalize data among clients in splitting\n",
    "        \"labels\": list(range(11)),  # list of label indices [0..10] for classes\n",
    "        \"clien_need_dist_opt\": False, # (unused flag, possibly if clients need distinct optimizers for distillation)\n",
    "        \"discri_model_out_len\": 1,  # output length for discriminator (binary output)\n",
    "        \"classify_model_out_len\": 11, # output length for classifier (11 classes)\n",
    "        \"sample_cnt\": 1000,         # number of samples per class (if using full dataset, here indicates the dataset was limited to 1000 per class)\n",
    "        \"random\": True,\n",
    "        \"vote\": True,\n",
    "        \"seed\": 7,                  # random seed for reproducibility\n",
    "        \"load_data_from_pickle\": False,\n",
    "        \"soft_label\": False,        # whether using soft labels directly (we use hard labels in voting)\n",
    "        \"num_after_float\": 4,\n",
    "        \"theta\": -1,                # threshold for unknown detection; -1 means use dynamic median-based threshold\n",
    "        \"split\": \"dile\",            # type of data split: \"dile\" for Dirichlet, \"equally\" for each-class-per-client, or \"iid\"\n",
    "        \"alpha_of_dile\": 0.1,       # alpha parameter for Dirichlet split\n",
    "    }\n",
    "    # Set random seed if specified\n",
    "    if configs[\"seed\"] is not None:\n",
    "        np.random.seed(configs[\"seed\"])\n",
    "    # Device: use GPU if available, else CPU\n",
    "    dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    # Prepare datasets for N-BaIoT\n",
    "    test_dataset, private_datasets, open_dataset = CreateDataset(configs, \"NBaIoT\")\n",
    "    # Initialize clients and server\n",
    "    device_names = [\n",
    "        \"Danmini_Doorbell/\", \"Ecobee_Thermostat/\", \"Philips_B120N10_Baby_Monitor/\",\n",
    "        \"Provision_PT_737E_Security_Camera/\", \"Provision_PT_838_Security_Camera/\",\n",
    "        \"SimpleHome_XCS7_1002_WHT_Security_Camera/\", \"SimpleHome_XCS7_1003_WHT_Security_Camera/\",\n",
    "        \"Ennio_Doorbell/\", \"Samsung_SNH_1011_N_Webcam/\",\n",
    "    ]\n",
    "    device_cnt = len(device_names)\n",
    "    clients = []\n",
    "    client_idx = 0\n",
    "    # Iterate through each IoT device and create its clients\n",
    "    for d_idx in range(device_cnt):\n",
    "        cur_device_client_cnt = GetDeviceClientCnt(device_names[d_idx], configs[\"device_client_cnt\"], configs[\"classify_model_out_len\"])\n",
    "        cur_device_private_datasets = private_datasets[d_idx]\n",
    "        for i in range(cur_device_client_cnt):\n",
    "            classify_model_out_len = configs[\"classify_model_out_len\"]\n",
    "            classify_model = GetNbaIotModel(classify_model_out_len)  # new classifier CNN for client\n",
    "            discri_model_out_len = configs[\"discri_model_out_len\"]\n",
    "            discri_model = GetNbaIotModel(discri_model_out_len)     # reuse CNN architecture for discriminator (binary output)\n",
    "            # Create a client object with its private data and models\n",
    "            client = Create_SSFL_IDS_Client(client_idx, cur_device_private_datasets[i], classify_model, classify_model_out_len, \n",
    "                                            configs[\"label_lr\"], discri_model, discri_model_out_len, configs[\"discri_lr\"])\n",
    "            clients.append(client)\n",
    "            client_idx += 1\n",
    "    # Initialize the server's global model (with same architecture as classifier)\n",
    "    server_model = GetNbaIotModel(configs[\"classify_model_out_len\"])\n",
    "    server = Create_SSFL_IDS_Server(server_model, configs[\"classify_model_out_len\"], clients, configs[\"dist_lr\"])\n",
    "    # Run the federated training process\n",
    "    SSFL_IDS(configs, dev, clients, server, test_dataset, open_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d451ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSFL_IDS_Client:\n",
    "    def __init__(self, idx, classify_dataset: torch.utils.data.Dataset,\n",
    "                 classify_model: nn.Module, classify_model_out_len, classify_lr: float,\n",
    "                 discri_model: nn.Module, discri_model_out_len, discri_lr: float):\n",
    "        # Save the models and dataset\n",
    "        self.classify_model = classify_model        # classifier model for known classes\n",
    "        self.classify_dataset = classify_dataset    # private labeled dataset (TensorDataset) for training\n",
    "        # Determine number of classes for classification (for multi-class vs binary)\n",
    "        self.class_cat = classify_model_out_len if classify_model_out_len > 1 else 2\n",
    "        # Count labeled samples per class in this client's dataset\n",
    "        self.each_class_cnt = [0] * self.class_cat\n",
    "        for _, label in self.classify_dataset:\n",
    "            # label.item() works if label tensor is 0-d or 1-d; ensure label is int\n",
    "            lbl = int(label.item()) if hasattr(label, \"item\") else int(label)\n",
    "            if lbl < len(self.each_class_cnt):\n",
    "                self.each_class_cnt[lbl] += 1\n",
    "        self.classify_lr = classify_lr\n",
    "        self.c_idx = idx\n",
    "        # Optimizer for classifier model\n",
    "        self.classify_opt = optim.Adam(self.classify_model.parameters(), lr=self.classify_lr)\n",
    "        # Save discriminator and its properties\n",
    "        self.discri_model = discri_model\n",
    "        self.discri_lr = discri_lr\n",
    "        self.discri_opt = optim.Adam(self.discri_model.parameters(), lr=self.discri_lr)\n",
    "        self.discri_model_out_len = discri_model_out_len\n",
    "        # Loss function for discriminator\n",
    "        if discri_model_out_len == 1:\n",
    "            self.discri_loss_func = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.discri_loss_func = nn.CrossEntropyLoss()\n",
    "        self.classify_model_out_len = classify_model_out_len\n",
    "        # Loss function for classifier (hard label loss)\n",
    "        if classify_model_out_len == 1:\n",
    "            self.hard_label_loss_func = nn.BCEWithLogitsLoss()\n",
    "            # If binary output, ensure dataset labels are float (double) for BCE loss\n",
    "            features, labels = self.classify_dataset[:]\n",
    "            labels = labels.double()\n",
    "            self.classify_dataset = torch.utils.data.TensorDataset(features, labels)\n",
    "        else:\n",
    "            self.hard_label_loss_func = nn.CrossEntropyLoss()\n",
    "        # Loss function for classifier (soft label loss) - used for distillation\n",
    "        self.soft_label_loss_func = SSFL_IDS_CELoss()\n",
    "\n",
    "class SSFL_IDS_CELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss for soft labels (used in knowledge distillation).\n",
    "    Essentially computes cross-entropy between a probability distribution target (soft labels) and the predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, pred_pro, target_tensor):\n",
    "        # pred_pro: model predictions (logits) for each class.\n",
    "        # target_tensor: soft target probabilities for each class.\n",
    "        pred_pro = F.log_softmax(pred_pro, dim=1)    # log-softmax of predictions\n",
    "        # Multiply by target probabilities and take negative sum (cross entropy)\n",
    "        out = -1 * pred_pro * target_tensor\n",
    "        return out.sum() / len(pred_pro)\n",
    "\n",
    "class SSFL_IDS_Server:\n",
    "    def __init__(self, model: nn.Module, model_out_len, clients, dist_lr: float):\n",
    "        self.model = model               # global model\n",
    "        self.clients = clients           # reference to list of clients (not used in training directly)\n",
    "        self.client_cnt = len(clients)\n",
    "        self.model_out_len = model_out_len\n",
    "        self.dist_lr = dist_lr\n",
    "        # Optimizer for server model (for distillation updates)\n",
    "        self.dist_opt = optim.Adam(self.model.parameters(), lr=self.dist_lr)\n",
    "        # Loss function for distillation on server\n",
    "        self.soft_label_loss_func = SSFL_IDS_CELoss()\n",
    "        # Loss for hard labels on server model (though server uses only soft labels in this code)\n",
    "        if model_out_len != 1:\n",
    "            self.hard_label_loss_func = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.hard_label_loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def Create_SSFL_IDS_Client(client_idx, private_dataset, classify_model, classify_model_out_len,\n",
    "                           lr, discri_model, discri_model_out_len, discri_lr):\n",
    "    \"\"\"Factory function to create a SSFL_IDS_Client instance.\"\"\"\n",
    "    client = SSFL_IDS_Client(client_idx, private_dataset, classify_model, classify_model_out_len,\n",
    "                              lr, discri_model, discri_model_out_len, discri_lr)\n",
    "    return client\n",
    "\n",
    "def Create_SSFL_IDS_Server(server_model, classify_model_out_len, clients, dist_lr):\n",
    "    \"\"\"Factory function to create a SSFL_IDS_Server instance.\"\"\"\n",
    "    server = SSFL_IDS_Server(server_model, classify_model_out_len, clients, dist_lr)\n",
    "    return server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4817a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(configs, dataset_name=\"NBaIoT\"):\n",
    "    if dataset_name == \"NBaIoT\":\n",
    "        return create_NBaIoT(configs)\n",
    "\n",
    "def create_NBaIoT(configs):\n",
    "    prefix = \"C:/Users/Iotworkstation/Desktop/majorproject/USSFL/data/nba_iot_1000/\"\n",
    "    # Device and attack type names as in the dataset directory\n",
    "    device_names = [\n",
    "        \"Danmini_Doorbell/\", \"Ecobee_Thermostat/\", \"Philips_B120N10_Baby_Monitor/\",\n",
    "        \"Provision_PT_737E_Security_Camera/\", \"Provision_PT_838_Security_Camera/\",\n",
    "        \"SimpleHome_XCS7_1002_WHT_Security_Camera/\", \"SimpleHome_XCS7_1003_WHT_Security_Camera/\",\n",
    "        \"Ennio_Doorbell/\", \"Samsung_SNH_1011_N_Webcam/\",\n",
    "    ]\n",
    "    attack_names = [\n",
    "        \"benign\", \"g_combo\", \"g_junk\", \"g_scan\", \"g_tcp\", \"g_udp\",\n",
    "        \"m_ack\", \"m_scan\", \"m_syn\", \"m_udp\", \"m_udpplain\"\n",
    "    ]\n",
    "    # If doing binary classification (classify_model_out_len==1), only use \"benign\" and combined \"attack\"\n",
    "    if configs[\"classify_model_out_len\"] == 1:\n",
    "        attack_names = [\"benign\", \"attack\"]\n",
    "    # Initialize containers for combined data\n",
    "    all_device_train_feature = None\n",
    "    all_device_train_label = None\n",
    "    all_device_open_feature = None\n",
    "    all_device_open_label = None\n",
    "    all_device_private_feature = []  # list of private features for each device (before splitting to clients)\n",
    "    all_device_private_label = []\n",
    "    all_device_test_feature = None\n",
    "    all_device_test_label = None\n",
    "    device_cnt = len(device_names)\n",
    "    # Loop over each device to read data\n",
    "    if not configs.get(\"load_data_from_pickle\", False):\n",
    "        for d_idx in range(device_cnt):\n",
    "            # Determine how many classes this device has (some devices might not have all attacks)\n",
    "            cur_device_class_cat = GetDeviceClassCat(device_names[d_idx], configs[\"classify_model_out_len\"])\n",
    "            train_filenames = []\n",
    "            test_filenames = []\n",
    "            # Prepare file list for this device's train and test data\n",
    "            for i in range(len(attack_names)):\n",
    "                if i < cur_device_class_cat:\n",
    "                    train_filename = prefix + device_names[d_idx] + attack_names[i] + \"_train.csv\"\n",
    "                    test_filename = prefix + device_names[d_idx] + attack_names[i] + \"_test.csv\"\n",
    "                    train_filenames.append(train_filename)\n",
    "                    test_filenames.append(test_filename)\n",
    "            # Read and aggregate all training data for this device\n",
    "            train_feature, train_label = GetAllFeatureLabel(train_filenames, configs[\"label_start_idx\"])\n",
    "            # Split into private and open sets for this device\n",
    "            private_feature, private_label, open_feature, open_label = SplitPrivateOpen(\n",
    "                train_feature, train_label,\n",
    "                configs[\"private_percent\"], configs[\"open_percent\"],\n",
    "                cur_device_class_cat, False\n",
    "            )\n",
    "            all_device_private_feature.append(private_feature)\n",
    "            all_device_private_label.append(private_label)\n",
    "            # Accumulate open data from this device into the global open pool\n",
    "            if all_device_open_feature is None:\n",
    "                all_device_open_feature = open_feature\n",
    "                all_device_open_label = open_label\n",
    "            else:\n",
    "                all_device_open_feature = np.concatenate((all_device_open_feature, open_feature), axis=0)\n",
    "                all_device_open_label = np.concatenate((all_device_open_label, open_label), axis=0)\n",
    "            # (all_device_train_feature/label are not extensively used except perhaps for debugging)\n",
    "            if all_device_train_feature is None:\n",
    "                all_device_train_feature = train_feature\n",
    "                all_device_train_label = train_label\n",
    "            # Read and aggregate all test data for this device\n",
    "            test_feature, test_label = GetAllFeatureLabel(test_filenames, configs[\"label_start_idx\"])\n",
    "            if all_device_test_feature is None:\n",
    "                all_device_test_feature = test_feature\n",
    "                all_device_test_label = test_label\n",
    "            else:\n",
    "                all_device_test_feature = np.concatenate((all_device_test_feature, test_feature), axis=0)\n",
    "                all_device_test_label = np.concatenate((all_device_test_label, test_label), axis=0)\n",
    "    # Scale the features using Min-Max normalization (fit on open data, apply to all)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_device_open_feature)\n",
    "    # Transform open and test features\n",
    "    all_device_open_feature = scaler.transform(all_device_open_feature)\n",
    "    all_device_open_feature = reshape_sample(all_device_open_feature)\n",
    "    open_dataset = GetDataset(all_device_open_feature, all_device_open_label)\n",
    "    open_dataset = ShuffleDataset(open_dataset)\n",
    "    all_device_test_feature = scaler.transform(all_device_test_feature)\n",
    "    all_device_test_feature = reshape_sample(all_device_test_feature)\n",
    "    test_dataset = GetDataset(all_device_test_feature, all_device_test_label)\n",
    "    # Split each device's private data among its clients\n",
    "    private_datasets = []\n",
    "    for d_idx in range(device_cnt):\n",
    "        cur_device_class_cat = GetDeviceClassCat(device_names[d_idx], configs[\"classify_model_out_len\"])\n",
    "        cur_device_client_cnt = GetDeviceClientCnt(device_names[d_idx], configs[\"device_client_cnt\"], configs[\"classify_model_out_len\"])\n",
    "        # Get this device's private features and labels\n",
    "        cur_device_private_feature = all_device_private_feature[d_idx]\n",
    "        cur_device_private_label = all_device_private_label[d_idx]\n",
    "        # Apply the same normalization to private data\n",
    "        cur_device_private_feature = scaler.transform(cur_device_private_feature)\n",
    "        cur_device_private_feature = reshape_sample(cur_device_private_feature)\n",
    "        # Partition the private data into client subsets according to the config\n",
    "        if configs[\"iid\"] == True:\n",
    "            # Fully IID split among clients\n",
    "            cur_device_private_datasets = SplitPrivate(cur_device_private_feature, cur_device_private_label,\n",
    "                                                      cur_device_client_cnt, cur_device_class_cat,\n",
    "                                                      iid=True, data_average=configs[\"data_average\"])\n",
    "        elif configs[\"split\"] == \"dile\":\n",
    "            # Dirichlet distribution split among clients\n",
    "            cur_device_private_datasets = DilSplitPrivate(cur_device_private_feature, cur_device_private_label,\n",
    "                                                         cur_device_client_cnt, cur_device_class_cat,\n",
    "                                                         configs[\"alpha_of_dile\"], configs[\"seed\"])\n",
    "        elif configs[\"split\"] == \"equally\":\n",
    "            # Equal class split (each class to specific clients)\n",
    "            cur_device_private_datasets = SplitPrivate(cur_device_private_feature, cur_device_private_label,\n",
    "                                                      cur_device_client_cnt, cur_device_class_cat,\n",
    "                                                      iid=False, data_average=configs[\"data_average\"])\n",
    "        else:\n",
    "            # Default to IID if unspecified\n",
    "            cur_device_private_datasets = SplitPrivate(cur_device_private_feature, cur_device_private_label,\n",
    "                                                      cur_device_client_cnt, cur_device_class_cat,\n",
    "                                                      iid=True, data_average=configs[\"data_average\"])\n",
    "        private_datasets.append(cur_device_private_datasets)\n",
    "    return test_dataset, private_datasets, open_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6068c69d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 115)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the main training execution for USSFL-IDS on N-BaIoT dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mSSFL_IDS_NBaIoT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mSSFL_IDS_NBaIoT\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m dev = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Prepare datasets for N-BaIoT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m test_dataset, private_datasets, open_dataset = \u001b[43mCreateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNBaIoT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Initialize clients and server\u001b[39;00m\n\u001b[32m     53\u001b[39m device_names = [\n\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDanmini_Doorbell/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEcobee_Thermostat/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPhilips_B120N10_Baby_Monitor/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mProvision_PT_737E_Security_Camera/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mProvision_PT_838_Security_Camera/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSimpleHome_XCS7_1002_WHT_Security_Camera/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSimpleHome_XCS7_1003_WHT_Security_Camera/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEnnio_Doorbell/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSamsung_SNH_1011_N_Webcam/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     58\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mCreateDataset\u001b[39m\u001b[34m(configs, dataset_name)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCreateDataset\u001b[39m(configs, dataset_name=\u001b[33m\"\u001b[39m\u001b[33mNBaIoT\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dataset_name == \u001b[33m\"\u001b[39m\u001b[33mNBaIoT\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_NBaIoT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mcreate_NBaIoT\u001b[39m\u001b[34m(configs)\u001b[39m\n\u001b[32m    191\u001b[39m             all_device_test_label = np.concatenate((all_device_test_label, test_label), axis=\u001b[32m0\u001b[39m)\n\u001b[32m    193\u001b[39m scaler = MinMaxScaler()\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_device_open_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m all_device_open_feature = scaler.transform(all_device_open_feature)\n\u001b[32m    197\u001b[39m all_device_open_feature = reshape_sample(all_device_open_feature)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iotworkstation\\Desktop\\majorproject\\USSFL\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:434\u001b[39m, in \u001b[36mMinMaxScaler.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iotworkstation\\Desktop\\majorproject\\USSFL\\venv\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1144\u001b[39m     estimator._validate_params()\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1147\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1148\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1149\u001b[39m     )\n\u001b[32m   1150\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iotworkstation\\Desktop\\majorproject\\USSFL\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:472\u001b[39m, in \u001b[36mMinMaxScaler.partial_fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    467\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMinMaxScaler does not support sparse input. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConsider using MaxAbsScaler instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m     )\n\u001b[32m    471\u001b[39m first_pass = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m data_min = np.nanmin(X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    480\u001b[39m data_max = np.nanmax(X, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iotworkstation\\Desktop\\majorproject\\USSFL\\venv\\Lib\\site-packages\\sklearn\\base.py:604\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    602\u001b[39m         out = X, y\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m    606\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iotworkstation\\Desktop\\majorproject\\USSFL\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:969\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    967\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    970\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    971\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    972\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m    976\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 115)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "# Run the main training execution for USSFL-IDS on N-BaIoT dataset\n",
    "SSFL_IDS_NBaIoT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29f7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
